# -*- coding: utf-8 -*-
"""Data Mining Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aaz-zth0axnIHMevlMazeB47IlS0mOAa
"""

# install the package in google collab
!pip install ydata-profiling

pip install pycountry

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import scale
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV
from sklearn.linear_model import ElasticNet, ElasticNetCV
from sklearn.metrics import mean_squared_error
import plotly.graph_objs as go
from plotly.offline import iplot
import pycountry

from sklearn.model_selection import GridSearchCV
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import (classification_report, f1_score,  recall_score,
                             roc_auc_score, balanced_accuracy_score)
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestClassifier

# for a more in depth EDA
from ydata_profiling import ProfileReport

"""1.  Number of Contacts: Users with a larger number of contacts may be more engaged with the platform's social aspects and less likely to churn.
2.   Plan Type: Users on certain plans may be more likely to churn. Higher-tier plans might have more features, which could increase user retention.
3. Notification Preferences: Users who have opted out of marketing notifications (both push and email) may be less engaged with the platform and more likely to churn.
4. Transaction Activity: Users who have not made transactions recently may be more likely to churn. This could be inferred from the "transactions_state" column.
5. Geographic Location: Churn rates may vary based on the user's country, city, or even merchant location (ea_merchant_city, ea_merchant_country). Certain regions may have different economic conditions or competition from other financial service providers, influencing churn rates.
6. Usage of Crypto Feature: Users who have unlocked the crypto feature may be more engaged with the platform and less likely to churn.
7. Device type: Maybe users who use a certain device are more likely to be churn users.

# **1. STARTING EDA**

# **Users exploration**
"""

pwd

users = pd.read_csv("users.csv")
users.head()

users.info()

number_columns = users.select_dtypes(include=['int64', 'float64'])
number_columns.mean()

# Notice that created_date is type object, lets convert it
users['created_date'] = pd.to_datetime(users['created_date'])
print('The last created user was {} and the first {}'.format(users['created_date'].max(), users['created_date'].min()))

# adding describe for a quick EDA, notice the num_referrals & num_successful_referrals column are 0 for mean, min, max meaning they have no values
users.describe()

#The plans are three: GOLD the best and Standard the worst
distinct_plan = users['plan'].unique()
print(distinct_plan)

distinct_user_settings_crypto_unlocked = users['user_settings_crypto_unlocked'].unique()
print("Unique user_settings_crypto_unlocked: {}".format(distinct_user_settings_crypto_unlocked))

distinct_attributes_notifications_marketing_push = users['attributes_notifications_marketing_push'].unique()
print("Unique attributes_notifications_marketing_push: {}".format(distinct_attributes_notifications_marketing_push))

distinct_attributes_notifications_marketing_email = users['attributes_notifications_marketing_email'].unique()
print("Unique attributes_notifications_marketing_email: {}".format(distinct_attributes_notifications_marketing_email))

distinct_num_referrals = users['num_referrals'].unique()
print("Unique num_referrals: {}".format(distinct_num_referrals))

distinct_num_successful_referrals = users['num_successful_referrals'].unique()
print("Unique num_successful_referrals: {}".format(distinct_num_successful_referrals))

#num_referrals and num_succeddful_referrals are all 0 so we can drop these two columns as they do not give us any information to distinguish churn users

distinct_country = users['country'].unique()
print(distinct_country)

distinct_city = users['city'].unique()
print(distinct_city)

print('The oldest user was born in {} and the youngest {}'.format(users['birth_year'].min(), users['birth_year'].max()))

plt.figure(figsize=(10, 6))
plt.hist(users['birth_year'], bins=range(users['birth_year'].min(), users['birth_year'].max() + 1), edgecolor='black')
plt.title('Distribution of Users per Birth Year')
plt.xlabel('Birth Year')
plt.ylabel('Number of Users')
plt.xticks(range(users['birth_year'].min(), users['birth_year'].max() + 1, 5))  # Set the step to 5 for example
plt.grid(True)
plt.show()

# Check for missing values-attributes_notifications_marketing_push and attributes_notifications_marketing_email have many null values
print(users.isnull().sum())

"""The below plot is to check outliers. num_contacts has a lot of outliers with the highest number being 3000. This could be real so I will not remove the outliers."""

users.iloc[:, 3:11].plot.box(figsize=(10,6))

users_filtered = users.drop(columns=['num_successful_referrals', 'num_referrals'])

# Histogram of a numerical column
plt.hist(users_filtered['num_contacts'])
plt.xlabel('Number of Contacts')
plt.ylabel('Frequency')
plt.title('Distribution of Number of Contacts')
plt.show()

# Scatter plot of two numerical columns
plt.scatter(users_filtered['birth_year'], users_filtered['num_contacts'])
plt.xlabel('Birth Year')
plt.ylabel('Number of Contacts')
plt.title('Relationship between Birth Year and Number of Contacts')
plt.show()

# Box plot of a numerical column grouped by a categorical column
sns.boxplot(x='plan', y='num_contacts', data=users_filtered)
plt.xlabel('Plan')
plt.ylabel('Number of Contacts')
plt.title('Distribution of Number of Contacts by Plan')
plt.show()

# correlations between user characteristics after dropping num_referrals & num_successful_referrals

users_filtered.corr(numeric_only = True)

sns.heatmap(users_filtered.corr(numeric_only = True),annot=True)

"""Weak correlations overall; somewhat medium correlation between marketing_push and marketing_email which makes intuitive sense as opting in for push will most likely opt in for email and vice versa. This could be because for instant the opting in for push / email are on the same screen, so users at that time can select either medium for opting in."""

# examine profile report
profile_users = ProfileReport(users_filtered, title = "Users Filtered Report")

profile_users

print("Summary of EDA")
print(" ")
print("1. High EU country presence with UK being #1 (1/3 users from there), and top 5 being in the EU")
print(" ")
print(users_filtered['country'].value_counts().head())
print(" ")
print("2. Regarding City, London (7.5% of users), Paris, Dublin, Warsaw and Bucharest are amongst top 5")
print(users_filtered['city'].value_counts().head())
print(" ")
print("3. User crypto setting highly unbalanced with only 18% having activated the feature")
print(" ")
print("4. User plan highly unbalanced with 93% belonging to the standard plan.")
print(" ")
print("5. Marketing push and marketing email have a high number of missing values. We will need to see how we can treat them during preprocessing")
print(" ")

"""# **Transactions exploration**"""

pwd

transactions = pd.read_csv("transactions.csv")
transactions.head()

transactions.shape

transactions.info()

"""The users who have not made recent transactions may be a churn user."""

transactions['created_date'] = pd.to_datetime(transactions['created_date'])
print('The latest transaction was {} and the first {}'.format(transactions['created_date'].max(), transactions['created_date'].min()))

number_columns = transactions.select_dtypes(include=['int64', 'float64'])
number_columns.describe()

# ea_cardholderpresence, ea_merchant_mcc, ea_merchant_city, ea_merchant_country have null values
print(transactions.isnull().sum())

distinct_transactions_type = transactions['transactions_type'].unique()
print('transactions_type: {}'.format(distinct_transactions_type))

distinct_transactions_currency = transactions['transactions_currency'].unique()
print('transactions_currency: {}'.format(distinct_transactions_currency))

distinct_transactions_state = transactions['transactions_state'].unique()
print('transactions_state: {}'.format(distinct_transactions_state))

distinct_ea_cardholderpresence = transactions['ea_cardholderpresence'].unique()
print('ea_cardholderpresence: {}'.format(distinct_ea_cardholderpresence))

#This column contains the Merchant Category Code (MCC) for the merchant involved in the transaction. The MCC is a four-digit code used to classify merchants by their primary business activities.
distinct_ea_merchant_mcc = transactions['ea_merchant_mcc'].unique()
print('ea_merchant_mcc: {}'.format(distinct_ea_merchant_mcc))

distinct_ea_merchant_city = transactions['ea_merchant_city'].unique()
print('ea_merchant_city: {}'.format(distinct_ea_merchant_city))

distinct_ea_merchant_country = transactions['ea_merchant_country'].unique()
print('ea_merchant_country: {}'.format(distinct_ea_merchant_country))

distinct_direction = transactions['direction'].unique()
print('direction: {}'.format(distinct_direction))

# the thought process was initially to keep the columns and fill them with with the Value of UNKNOWN
# However these values are close to 50% of the column values
# We will proceed with removing these columns and keep a mental note to perhaps repeat in the future with UNKNOWN
transactions.drop(columns = ["ea_cardholderpresence","ea_merchant_mcc","ea_merchant_city","ea_merchant_country"],inplace=True)

#transaction to be 0 seems wrong
print('The largest amount was {} and the smallest {}'.format(transactions['amount_usd'].max(), transactions['amount_usd'].min()))

#it is weird that those transactions were completed but we look for churn users and transactions even with 0 dollars are important to identify churn users so we will not remove them.
print(transactions[(transactions['amount_usd'] == 0.0) & (transactions['transactions_state'] == 'COMPLETED')])

# it seems unbelievable that one transaction was 74641551593.26 usd
transactions.iloc[:, 1:7].plot.box(figsize=(10,6))

z_scores = np.abs(stats.zscore(transactions['amount_usd']))
threshold = 3
transactions = transactions[(z_scores < threshold)]

transactions.iloc[:, 1:7].plot.box(figsize=(10,6))

transactions.info()

transactions.iloc[:,1:-2] # select all the transactions columns we want without having the transaction id, created date and user id

transactions.iloc[:,1:-2]["direction"].value_counts()

transactions.iloc[:,1:-2]["transactions_state"].value_counts()

transactions.iloc[:,1:-2]["transactions_currency"].value_counts()

transactions.iloc[:,1:-2]["transactions_type"].value_counts()

#profile_transactions = ProfileReport(transactions.iloc[:,1:-2], title = "Transactions Report")

#profile_transactions
# crashes in Google Collab , goes over >12.7 GB

"""Based on the above, without being able to run profilereport on transactions we have the following summary:

*   Majority are outbound in terms of direction
*   Card payments are the majority followed by ransfer and topup
* Completed make the majority of transactions
* EUR and GBP make the majority of currency transactions

# **Notifications exploration**
"""

notifications = pd.read_csv("notifications.csv")
notifications.head()

notifications.info()

notifications['created_date'] = pd.to_datetime(notifications['created_date'])
print('The latest notifications was {} and the first {}'.format(notifications['created_date'].max(), notifications['created_date'].min()))

distinct_reason = notifications['reason'].unique();
distinct_channel = notifications['channel'].unique();
distinct_status = notifications['status'].unique();
print(distinct_reason);
print(distinct_channel);
print(distinct_status);

print(notifications.isnull().sum())

notifications.groupby("user_id")["reason"].count().describe()

notifications.groupby("user_id")["reason"].count().reset_index().boxplot()

"""The middle 50% of users that have received a notification, have 4-8 notifications. The top 25% have above 8, with the max value being 289 (intuitively this is probably a user that frequently engages with the service)"""

notifications["reason"].value_counts()

"""Most notifications are either for promo or reengagement."""

notifications["channel"].value_counts() # farely balanced between email and push, with sms being the minority

notifications["status"].value_counts() # majority are sent. We keep the failed as well as the notification failing may provide an indication on churn (for example having deleted the app)

"""# **Devices exploration**"""

devices = pd.read_csv("devices.csv")
devices.head()

devices.info()

distinct_brand = devices['brand'].unique()
print(distinct_brand)

print(devices.isnull().sum())

# doing a value counts which will indicate the type of phone each user has, balanced dataset. We will keep the unknown
devices["brand"].value_counts()

devices["brand"].value_counts().plot.pie(y="count")

"""# Identify key relationships

Number of Contacts: Users with a larger number of contacts may be more engaged with the platform's social aspects and less likely to churn.
"""

min_num_contacts = users['num_contacts'].min();
mean_num_contacts = users['num_contacts'].mean();
max_num_contacts = users['num_contacts'].max();
print('Min number of contacts: {}, mean {}, max {}'.format(min_num_contacts, mean_num_contacts, max_num_contacts));

quantile_value_least = users['num_contacts'].quantile(0.05)
quantile_value_most = users['num_contacts'].quantile(0.95)

# Filter users who have number of contacts less than or equal to the quantile value
top_5_percent_least_contacts_users = users[users['num_contacts'] <= quantile_value_least]
# Filter users who have number of contacts more than or equal to the quantile value
top_5_percent_most_contacts_users = users[users['num_contacts'] >= quantile_value_most]

print("Users with the least amount of number of contacts (top 5%) with the least having {} and the most {}".format(
    top_5_percent_least_contacts_users['num_contacts'].min(),
    top_5_percent_least_contacts_users['num_contacts'].max()
))

print("Users with the most amount of number of contacts (top 5%) with the least having {} and the most {}".format(
    top_5_percent_most_contacts_users['num_contacts'].min(),
    top_5_percent_most_contacts_users['num_contacts'].max()
))

gold_plan_users = users[users['plan'] == 'GOLD']
silver_plan_users = users[users['plan'] == 'SILVER']
standard_plan_users = users[users['plan'] == 'STANDARD']

min_num_contacts_gold = gold_plan_users['num_contacts'].min();
mean_num_contacts_gold = gold_plan_users['num_contacts'].mean();
max_num_contacts_gold = gold_plan_users['num_contacts'].max();
print('Gold plan users: Min number of contacts: {}, mean {}, max {}'.format(min_num_contacts_gold, mean_num_contacts_gold, max_num_contacts_gold));

min_num_contacts_silver = silver_plan_users['num_contacts'].min();
mean_num_contacts_silver = silver_plan_users['num_contacts'].mean();
max_num_contacts_silver = silver_plan_users['num_contacts'].max();
print('Silver plan users: Min number of contacts: {}, mean {}, max {}'.format(min_num_contacts_silver, mean_num_contacts_silver, max_num_contacts_silver));

min_num_contacts_standard = standard_plan_users['num_contacts'].min();
mean_num_contacts_standard = standard_plan_users['num_contacts'].mean();
max_num_contacts_standard = standard_plan_users['num_contacts'].max();
print('Standard plan users: Min number of contacts: {}, mean {}, max {}'.format(min_num_contacts_standard, mean_num_contacts_standard, max_num_contacts_standard));

# Data
plan_names = ['Gold', 'Silver', 'Standard']
mean_contacts = [mean_num_contacts_gold, mean_num_contacts_silver, mean_num_contacts_standard]

# Plotting
x = range(len(plan_names))
width = 0.2

plt.bar([i + width for i in x], mean_contacts, width=width, label='Mean Contacts')

plt.xlabel('Plan')
plt.ylabel('Number of Mean Contacts')
plt.title('Number of Mean Contacts by Plan')
plt.xticks([i + width for i in x], plan_names)
plt.legend()

plt.show()

"""Having a gold plan usually means they have more contacts, the mean is twice as many as standard users. We see that the more expensive a plan is the more contacts a user has. However there are gold plan users with 0 contacts and Standard plan users with many contacts. Just from these two we can't say if a user is a churn user."""

notification_counts = notifications['user_id'].value_counts()
quantile_value_least = notification_counts.quantile(0.05)
least_notifications_users = notification_counts[notification_counts <= quantile_value_least].index.tolist()
quantile_value_most = notification_counts.quantile(0.95)
most_notifications_users = notification_counts[notification_counts >= quantile_value_most].index.tolist()

print("Users with the least amount of notifications (top 5%):")
print(least_notifications_users)

print("\nUsers with the most amount of notifications (top 5%):")
print(most_notifications_users)

#Filter the users with the most notifications and find the num_contacts and plan they have.
filtered_users_most_notifications = users[users['user_id'].isin(most_notifications_users)]

print("Users with the most amount of notifications (top 5%):")
print("They have mean amount of number contacts: {}".format(filtered_users_most_notifications['num_contacts'].mean()))
print("Gold plan user: {}".format(filtered_users_most_notifications[filtered_users_most_notifications['plan'] == 'GOLD'].shape[0]/filtered_users_most_notifications.shape[0]))
print("Silver plan user: {}".format(filtered_users_most_notifications[filtered_users_most_notifications['plan'] == 'SILVER'].shape[0]/filtered_users_most_notifications.shape[0]))
print("Standard plan user: {}".format(filtered_users_most_notifications[filtered_users_most_notifications['plan'] == 'STANDARD'].shape[0]/filtered_users_most_notifications.shape[0]))

#Filter the users with the least notifications and find the num_contacts and plan they have.
filtered_users_least_notifications = users[users['user_id'].isin(least_notifications_users)]

print("Users with the least amount of notifications (top 5%):")
print("They have mean amount of number contacts: {}".format(filtered_users_least_notifications['num_contacts'].mean()))
print("Gold plan user: {}".format(filtered_users_least_notifications[filtered_users_least_notifications['plan'] == 'GOLD'].shape[0]/filtered_users_least_notifications.shape[0]))
print("Silver plan user: {}".format(filtered_users_least_notifications[filtered_users_least_notifications['plan'] == 'SILVER'].shape[0]/filtered_users_least_notifications.shape[0]))
print("Standard plan user: {}".format(filtered_users_least_notifications[filtered_users_least_notifications['plan'] == 'STANDARD'].shape[0]/filtered_users_least_notifications.shape[0]))

# Data
plans = ['Gold', 'Silver', 'Standard']
most_notifications_percentage = [
    filtered_users_most_notifications[filtered_users_most_notifications['plan'] == 'GOLD'].shape[0] / filtered_users_most_notifications.shape[0],
    filtered_users_most_notifications[filtered_users_most_notifications['plan'] == 'SILVER'].shape[0] / filtered_users_most_notifications.shape[0],
    filtered_users_most_notifications[filtered_users_most_notifications['plan'] == 'STANDARD'].shape[0] / filtered_users_most_notifications.shape[0]
]
least_notifications_percentage = [
    filtered_users_least_notifications[filtered_users_least_notifications['plan'] == 'GOLD'].shape[0] / filtered_users_least_notifications.shape[0],
    filtered_users_least_notifications[filtered_users_least_notifications['plan'] == 'SILVER'].shape[0] / filtered_users_least_notifications.shape[0],
    filtered_users_least_notifications[filtered_users_least_notifications['plan'] == 'STANDARD'].shape[0] / filtered_users_least_notifications.shape[0]
]

# Plotting
x = np.arange(len(plans))  # Convert x to array
width = 0.35

fig, ax = plt.subplots()
rects1 = ax.bar(x - width/2, most_notifications_percentage, width, label='Most Notifications (top 5%)')
rects2 = ax.bar(x + width/2, least_notifications_percentage, width, label='Least Notifications (top 5%)')

ax.set_ylabel('Percentage of Users')
ax.set_title('Percentage of Users by Plan and Notification Amount')
ax.set_xticks(x)
ax.set_xticklabels(plans)
ax.legend()

def autolabel(rects):
    """Attach a text label above each bar in *rects*, displaying its height."""
    for rect in rects:
        height = rect.get_height()
        ax.annotate('{}'.format(round(height*100, 2)),
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')

autolabel(rects1)
autolabel(rects2)

fig.tight_layout()

plt.show()

"""The more notifications a user has, more likely to have more number contacts and the percentage of having a golden or silver plan is much more."""

transactions_counts = transactions['user_id'].value_counts()
quantile_value_least = transactions_counts.quantile(0.05)
least_transactions_users = transactions_counts[transactions_counts <= quantile_value_least].index.tolist()
quantile_value_most = transactions_counts.quantile(0.95)
most_transactions_users = transactions_counts[transactions_counts >= quantile_value_most].index.tolist()

#Filter the users with the most transactions and find the num_contacts and plan they have.
filtered_users_most_transactions = users[users['user_id'].isin(most_transactions_users)]

print("Users with the most amount of transactions (top 5%):")
print("They have mean amount of number contacts: {}".format(filtered_users_most_transactions['num_contacts'].mean()))
print("Gold plan user: {}".format(filtered_users_most_transactions[filtered_users_most_transactions['plan'] == 'GOLD'].shape[0]/filtered_users_most_transactions.shape[0]))
print("Silver plan user: {}".format(filtered_users_most_transactions[filtered_users_most_transactions['plan'] == 'SILVER'].shape[0]/filtered_users_most_transactions.shape[0]))
print("Standard plan user: {}".format(filtered_users_most_transactions[filtered_users_most_transactions['plan'] == 'STANDARD'].shape[0]/filtered_users_most_transactions.shape[0]))

#Filter the users with the least transactions and find the num_contacts and plan they have.
filtered_users_least_transactions = users[users['user_id'].isin(least_transactions_users)]

print("Users with the least amount of transactions (top 5%):")
print("They have mean amount of number contacts: {}".format(filtered_users_least_transactions['num_contacts'].mean()))
print("Gold plan user: {}".format(filtered_users_least_transactions[filtered_users_least_transactions['plan'] == 'GOLD'].shape[0]/filtered_users_least_transactions.shape[0]))
print("Silver plan user: {}".format(filtered_users_least_transactions[filtered_users_least_transactions['plan'] == 'SILVER'].shape[0]/filtered_users_least_transactions.shape[0]))
print("Standard plan user: {}".format(filtered_users_least_transactions[filtered_users_least_transactions['plan'] == 'STANDARD'].shape[0]/filtered_users_least_transactions.shape[0]))

# Data
plans = ['Gold', 'Silver', 'Standard']
most_transactions_percentage = [
    filtered_users_most_transactions[filtered_users_most_transactions['plan'] == 'GOLD'].shape[0] / filtered_users_most_transactions.shape[0],
    filtered_users_most_transactions[filtered_users_most_transactions['plan'] == 'SILVER'].shape[0] / filtered_users_most_transactions.shape[0],
    filtered_users_most_transactions[filtered_users_most_transactions['plan'] == 'STANDARD'].shape[0] / filtered_users_most_transactions.shape[0]
]
least_transactions_percentage = [
    filtered_users_least_transactions[filtered_users_least_transactions['plan'] == 'GOLD'].shape[0] / filtered_users_least_transactions.shape[0],
    filtered_users_least_transactions[filtered_users_least_transactions['plan'] == 'SILVER'].shape[0] / filtered_users_least_transactions.shape[0],
    filtered_users_least_transactions[filtered_users_least_transactions['plan'] == 'STANDARD'].shape[0] / filtered_users_least_transactions.shape[0]
]

# Plotting
x = np.arange(len(plans))  # Convert x to array
width = 0.35

fig, ax = plt.subplots()
rects1 = ax.bar(x - width/2, most_transactions_percentage, width, label='Most Transactions (top 5%)')
rects2 = ax.bar(x + width/2, least_transactions_percentage, width, label='Least Transactions (top 5%)')

ax.set_ylabel('Percentage of Users')
ax.set_title('Percentage of Users by Plan and Transaction Amount')
ax.set_xticks(x)
ax.set_xticklabels(plans)
ax.legend()

def autolabel(rects):
    """Attach a text label above each bar in *rects*, displaying its height."""
    for rect in rects:
        height = rect.get_height()
        ax.annotate('{}'.format(round(height*100, 2)),
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')

autolabel(rects1)
autolabel(rects2)

fig.tight_layout()

plt.show()

"""Again we see the more transactions a user has, more likely to have more number contacts and the percentage of having a golden or silver plan is much more."""

user_transactions = pd.merge(users, transactions_counts, left_on='user_id', right_index=True, how='left')

user_transactions['count'].max()

# Import necessary libraries
import plotly.graph_objects as go
import pycountry

def featureWorldMap(feature, title):
    '''
    Returns a world heatmap of a given feature
    '''
    # Convert ISO-3166-1 alpha-2 country codes to alpha-3
    iso3_codes = [pycountry.countries.get(alpha_2=code).alpha_3 for code in user_transactions['country']]

    # Define the color scale and its corresponding ranges
    color_scale = [
        [0, "red"],             # 0-5
        [5/3381, "lightcoral"], # 5-20
        [20/3381, "palegreen"], # 20-50
        [50/3381, "green"],     # 30-infinity
        [1, "darkgreen"]
    ]

    # Create the choropleth map figure
    return go.Figure(
        data = {
            'type':'choropleth',
            'locations': iso3_codes,
            'locationmode': 'ISO-3',
            'colorscale': color_scale,
            'z': user_transactions[feature],
            'colorbar': {'title': title},
            'marker': {
                'line': {
                    'color': 'rgb(255,255,255)',
                    'width': 2
                }
            }
        },
        layout = {
            'geo': {
                'scope': 'world',
            }
        }
    )

featureWorldMap("count", "Number of transactions")

"""# Cutoff period to define churned users"""

# Regarding cutoff_date lets see the spread of frequency users have regarding transaction
transactions.head()

transactions.info()
# change created date to be recognized as tstamp

transactions["created_date"]=pd.to_datetime(transactions["created_date"])

"""What is a churned user? We can define it as:

*   Users that have not made a transaction ever (count of transactions = 0)
*   Users with > 0 transactions, but have not made a transaction in x amount of time

Side note: in terms of churned behavior, we do not care about the status of the transaction (failed/ success) as engaging with the application via a transaction even if it failed means the user engaged with the app.

Additionally in terms of churned behavior, we do not care of the direction of the transaction as our aim is predicting if a user will stop using the
company's services. Even in the case of the inbound transaction the user is technically using the company's services.




"""

max_tr_date_by_userid= transactions.groupby("user_id")["created_date"].max().reset_index() #per user date of most recent transaction by user

max_tr_date_by_userid.rename(columns={
    "created_date":"most_recent_date"
},inplace=True)
#rename column to most_recent_date

max_tr_date_by_userid

cmax_tr_date =transactions["created_date"].max() # date of most recent transaction

cmax_tr_date

transactions_w_recentdate=transactions.merge(max_tr_date_by_userid,on="user_id")

max_tr_date_by_userid["days_since_last_transaction"]=max_tr_date_by_userid["most_recent_date"].apply(lambda x: (cmax_tr_date - x).days)

max_tr_date_by_userid["days_since_last_transaction"].describe()

max_tr_date_by_userid.boxplot(column=["days_since_last_transaction"])

"""We see that our "lowest" quartile (the quartile with users who have had the greatest amount of days since the last transaction, is >=92 days). Thus we can establish a cutoff point for churned users to be 3 months

Let's also explore the quartile range for each user's days since last transaction belonging to a plan as an extra exploration layer
"""

sns.violinplot(x=users.merge(max_tr_date_by_userid,on="user_id")["plan"],y=users.merge(max_tr_date_by_userid,on="user_id")["days_since_last_transaction"],order=["STANDARD","SILVER","GOLD"])

"""What we see is that there is a much lower variation of days since last transaction as we move from standard -> silver -> gold. Additionally, there is a higher distribution of users that are closer to 0 the higher the plan, which intuitively tells us that a user that in a higher plan is much less likely to be considered as opted out"""

users_last_transaction_days = users.merge(max_tr_date_by_userid, on="user_id")

# Filter users based on the conditions
users_lt_90 = users_last_transaction_days[users_last_transaction_days["days_since_last_transaction"] < 90]
users_gt_90 = users_last_transaction_days[users_last_transaction_days["days_since_last_transaction"] > 90]

count_lt_90 = users_lt_90["plan"].value_counts()
count_gt_90 = users_gt_90["plan"].value_counts()

# Calculate the total number of users in each group
total_lt_90 = len(users_lt_90)
total_gt_90 = len(users_gt_90)

# Define the width of each bar
bar_width = 0.35

# Define the x positions for the bars
x_lt_90 = np.arange(len(count_lt_90))
x_gt_90 = np.arange(len(count_gt_90)) + bar_width

# Create bar chart
plt.figure(figsize=(10, 6))
bars_lt_90 = plt.bar(x_lt_90, count_lt_90.values, color="skyblue", width=bar_width, label="< 90 days")
bars_gt_90 = plt.bar(x_gt_90, count_gt_90.values, color="salmon", width=bar_width, label="> 90 days")
plt.xlabel("Plan")
plt.ylabel("Number of Users")
plt.title("Number of Users by Plan and Days Since Last Transaction")
plt.xticks(ticks=np.arange(len(count_lt_90)) + bar_width / 2, labels=count_lt_90.index)
plt.legend()

# Add percentage values on top of each bar for < 90 days
def add_percentage_labels_lt_90(bars):
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width() / 2, height,
                 '{:.2f}%'.format((height / total_lt_90) * 100),
                 ha='center', va='bottom')

# Add percentage values on top of each bar for > 90 days
def add_percentage_labels_gt_90(bars):
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width() / 2, height,
                 '{:.2f}%'.format((height / total_gt_90) * 100),
                 ha='center', va='bottom')

add_percentage_labels_lt_90(bars_lt_90)
add_percentage_labels_gt_90(bars_gt_90)

plt.show()

sns.violinplot(x=devices.merge(max_tr_date_by_userid,on="user_id")["brand"],y=devices.merge(max_tr_date_by_userid,on="user_id")["days_since_last_transaction"],order=["Android","Apple","Unknown"])

"""What we see is that there is a lower variation of days since last transaction as we move from Android -> Apple -> Unknown. However, the distribution of users is almost the same, which tells us that the device does not tell us if a user will be a churned user."""

# Define the cutoff date, 3 months seems to be enough to say this is a churn user
cutoff_date = pd.to_datetime('2019-02-12')

# Filter users who have not had a transaction after the cutoff date
inactive_users = transactions[transactions['created_date'] <= cutoff_date]['user_id'].unique()

print("Users who have not had a transaction after 2019-04-12:")
print(inactive_users)

max_days_since_last_transaction = users_last_transaction_days['days_since_last_transaction'].max()
print("Maximum number of days since last transaction:", max_days_since_last_transaction)

#https://levelup.gitconnected.com/plotting-choropleth-maps-in-python-b74c53b8d0a6
def featureWorldMapDaysSinceLastTransaction(feature, title):
    '''
    Returns a world heatmap of a given feature
    '''
    iso3_codes = [pycountry.countries.get(alpha_2=code).alpha_3 for code in users_last_transaction_days['country']]

    return go.Figure(
        data = {
            'type':'choropleth',
            'locations':iso3_codes,
            'locationmode':'ISO-3',
            'colorscale': [
                [0, "darkgreen"],   # 0-10
                [30/498, "green"],    # 30-60
                [60/498, "palegreen"], # 60-90
                [90/498, "red"], # 90-infinity
                [1, "red"]
            ],
            'z':users_last_transaction_days[feature],
            'colorbar':{'title': title},
            'marker': {
                'line': {
                    'color':'rgb(255,255,255)',
                    'width':2
                }
            }
        },
        layout = {
          'geo':{
              'scope':'world',
          }
        }
    )

featureWorldMapDaysSinceLastTransaction("days_since_last_transaction", "Days since last transaction")

"""# **2. Initial preprocessing**

This part will serve to clean the data, bring the tables together, as well as define churned behavior. Lets start with the users_filtered table, which we have already dropped two empty columns from.

### users_filtered
"""

users_filtered.info()
# rerunning user info, note we can create a new column based on birth_year which will be the user age
# we can create a user_age column based on birth_year
# we can create an age_in_service column based on created date;
# however this will require us to know the timestamp of the latest transaction in the service to be able to use that as a max value (keep a mental note for later)
# we can create a recency of account creation based on created_date, which will count the days of each user based on the most recent created date value

# Notice that created_date is type object, lets convert it
users_filtered["created_date"]=pd.to_datetime(users_filtered["created_date"])

# we can also create created_day, created_month, created_ year from created date

users_filtered["created_day_of_month"]=users_filtered["created_date"].dt.day
users_filtered["created_month_of_year"]=users_filtered["created_date"].dt.month
users_filtered["created_year"]=users_filtered["created_date"].dt.year

users_filtered.info()

"""How will we handle the missing values from marketing push and marketing email?
We could use the route of knn imputer, however:
*  with only 15.5K records, and 33% of them missing I'm doubtful of how precise the knn imputer can be
*   there is a low correlation between these two columns and any of the rest columns
*   we do not have a great amount of columns to infer relationships.

As such, I feel imputing the values is not going to be beneficial and I believe the most appropriate course of action is to remove the columns entirely

Additionally, we had both values 1 (indicating enabled) and 0 (indicating disabled) present. If we had only values 1, we could assume that the null values = 0 with a higher likelihood in order to replace the values with 0. However, this is not the case here.
"""

users_filtered.drop(columns=["attributes_notifications_marketing_push","attributes_notifications_marketing_email"],inplace=True)

users_filtered.head()

users_filtered.info()

"""### transactions"""

transactions.head()

transactions["transactions_state"].unique()

transactions["direction"].unique()

transactions_2 = transactions.copy() # retain original dataframe

transactions_2_sorted=transactions_2.sort_values(by=["user_id","created_date"],ascending=[True,True]) # we sort the dataframe by user id then by created date
# we are thinking about inserting a transaction logic

transactions_2_sorted

# we want to evaluate the balance of the user essentially
# we define a logic regarding the usd amount based on whether it is an inbound or outbound transaction and the state of transaction
# this logic will return the value of amount_usd positive, negative or simply 0
# for example if the transaction is failed or pending, then the balance of the user has not really changed
# if the transaction is complete then the direction of the transaction is respected (inbound means positive amount usd, outbound means negative amount usd)

def amount_calculator_logic(x):
  if x["direction"] == "INBOUND":
    if x["transactions_state"] == "COMPLETED":
      return x["amount_usd"]
    elif x["transactions_state"] == "REVERTED" or x["transactions_state"] == "CANCELLED":
      return 0 - x["amount_usd"]
    else:
      return 0
  elif x["direction"] == "OUTBOUND":
    if x["transactions_state"] == "COMPLETED":
      return 0 - x["amount_usd"]
    elif x["transactions_state"] == "REVERTED" or x["transactions_state"] == "CANCELLED":
      return x["amount_usd"]
    else:
      return 0

transactions_2["amount_usd_logic"] =transactions_2_sorted.apply(amount_calculator_logic,axis=1) # we apply the function on a new column

# now we can essentially calculate the balance per user and the count of transactions by user
userid_totalamount= transactions_2.groupby("user_id").agg({
    "amount_usd":np.sum,
    "transaction_id":"count"
}).reset_index().rename(columns={
    "amount_usd":"amount_usd_total",
    "transaction_id": "count_of_transactions"
})

userid_totalamount
# note the table shows the amount from a user perspective, example user_0's balance negative balance means he has done a greater value of outbound transactions
# now we can add this to the users_filtered table

users_filtered=users_filtered.merge(userid_totalamount,how="left",on="user_id") # mergingon user_id

users_filtered.shape

users_filtered.head()

user_count_transactions_state=pd.crosstab(transactions_2['user_id'], transactions_2['transactions_state']).reset_index()
 # we are essentially counting the types of different transactions per user
 # we have already added the total count of transactions, however this way we have more information on a per user lever, for example the number of failed transactions

# now its time to merge with the users_filtered
users_filtered=users_filtered.merge(user_count_transactions_state,how="left", on="user_id")

users_filtered.head(3)

users_filtered.shape

max_tr_date_by_userid.head() # remember our previous table that includes the days_since last transaction?
# we can add the days_since column to our dataframe as well

max_tr_date_by_userid.drop("most_recent_date",axis=1,inplace=True)

users_filtered=users_filtered.merge(max_tr_date_by_userid,how="left",on="user_id") # merging again

users_filtered.shape

# another thing we want to do is obtain the latest transaction type, currency, amount usd logic etc for each user
# a way to do it is to make sure we have the df sorted by user id, then by created date in descending order.
# then we assign a rank on a per user id, date level where the most recent date will receive a 1
# we will then filter the dataframe for rank == 1
# we will then merge based on the user id with the users_filtered
transactions_2_sorted=transactions_2.sort_values(by=["user_id","created_date"],ascending=[True,False])
transactions_2_sorted.head()

transactions_2_sorted["rank"]=transactions_2_sorted.groupby("user_id")["created_date"].rank(method="first",ascending=False)
# we create a rank column, groupby user id, and then rank the date assigning a rank 1 to the latest

per_userid_latest_transactions = transactions_2_sorted[transactions_2_sorted["rank"] == 1] # we filter to get only the rank ==1

# we drop the columns transaction_id, amount_usd (since we have amount_usd_logic) and rank
per_userid_latest_transactions=per_userid_latest_transactions.drop(columns=["transaction_id","amount_usd","rank"])

per_userid_latest_transactions.head()

per_userid_latest_transactions.rename(
    columns={
        "created_date": "last_transaction_date",
        "transactions_type" : "last_transaction_type",
        "transactions_currency" : "last_transaction_currency",
        "transactions_state" : "last_transaction_state",
        "direction" : "last_transaction_direction"
    }
,inplace=True) # give different name to the date column as users_filtered already has a column with that name

# we can remerge now
users_filtered=users_filtered.merge(per_userid_latest_transactions,how="left",on="user_id")

users_filtered.info()
# notice that we have missing values, which are the users that did not complete a single transaction

users_filtered.columns

# columns - count of transactions until REVERTED can be filled with 0
users_filtered[['count_of_transactions', 'CANCELLED', 'COMPLETED', 'DECLINED', 'FAILED','PENDING', 'REVERTED']]=users_filtered[['count_of_transactions', 'CANCELLED', 'COMPLETED', 'DECLINED', 'FAILED','PENDING', 'REVERTED']].fillna(0)

users_filtered.info()
# days since last transaction can be given an extremely high value such as 999999
# transaction type -> direction can be given value None
# last transaction date can be given for now the most recent transaction date (we want to create a life in service column based on the difference of last trans date and created_date)
# amount_usd_logic and amount usd total given a 0

users_filtered.isnull().sum(axis=0)

(cmax_tr_date - users_filtered["created_date"]).dt.days

#based on the approach you want comment whichever of those two rows
#users_filtered["days_since_last_transaction"]=users_filtered["days_since_last_transaction"].fillna(999999)
users_filtered["days_since_last_transaction"]=users_filtered["days_since_last_transaction"].fillna((cmax_tr_date - users_filtered["created_date"]).dt.days)

users_filtered[['last_transaction_type', 'last_transaction_currency', 'last_transaction_state','last_transaction_direction']]=users_filtered[['last_transaction_type', 'last_transaction_currency', 'last_transaction_state','last_transaction_direction']].fillna("None")

#based on the approach you want comment whichever of those two rows
#users_filtered["last_transaction_date"]=users_filtered["last_transaction_date"].fillna(cmax_tr_date)
users_filtered["last_transaction_date"]=users_filtered["last_transaction_date"].fillna(users_filtered["created_date"])

users_filtered[["amount_usd_logic","amount_usd_total"]]=users_filtered[["amount_usd_logic","amount_usd_total"]].fillna(0)

users_filtered.info() # no nulls are present

users_filtered["age_in_service"]=(users_filtered["last_transaction_date"] - users_filtered["created_date"]).dt.days

users_filtered.rename(
    columns={
        "CANCELLED": "num_CANCELLED_transactions",
        "COMPLETED" : "num_COMPLETED_transactions",
        "DECLINED" : "num_DECLINED_transactions",
        "FAILED" : "num_FAILED_transactions",
        "PENDING" : "num_PENDING_transactions",
        "REVERTED" : "num_REVERTED_transactions",
        "amount_usd_logic" : "last_amount_usd_transaction"
    }
,inplace=True)

"""### notifications"""

notifications.info() # no null values

notifications # we will get a count of notifications by user, as well as the different counts per reason, channel status on a per user level

notif_count_by_user=notifications.groupby("user_id")["status"].count().reset_index().rename(columns={"status":"count of notifications"})

notif_count_by_user_reason=pd.crosstab(notifications['user_id'], notifications["reason"]).reset_index()

notif_count_by_user_channel=pd.crosstab(notifications['user_id'], notifications["channel"]).reset_index()

notif_count_by_user_status=pd.crosstab(notifications['user_id'], notifications["status"]).reset_index()

# time to merge

users_filtered=users_filtered.merge(notif_count_by_user,how="left",on="user_id")

users_filtered=users_filtered.merge(notif_count_by_user_reason,how="left",on="user_id")

users_filtered=users_filtered.merge(notif_count_by_user_channel,how="left",on="user_id")

users_filtered=users_filtered.merge(notif_count_by_user_status,how="left",on="user_id")

users_filtered.shape

notifications_sorted=notifications.sort_values(by=["user_id","created_date"],ascending=[True,False])# now we can repeat what we did for transactions, ie get the latest notification per user

notifications_sorted["rank"]=notifications_sorted.groupby("user_id")["created_date"].rank(method="first",ascending=False)

notifications_sorted_rank_1=notifications_sorted[notifications_sorted["rank"] == 1]

notifications_sorted_rank_1.head()

notifications_sorted_rank_1=notifications_sorted_rank_1.rename(
    columns={
        "created_date":"latest_notification_date",
        "reason":"latest_notification_reason",
        "channel":"latest_notification_channel",
        "status":"latest_notification_status"
        }
    )

users_filtered=users_filtered.merge(notifications_sorted_rank_1.iloc[:,:-1],how="left",on="user_id")

users_filtered.info()

users_filtered[['count of notifications',
       'BLACK_FRIDAY', 'BLUE_TUESDAY', 'ENGAGEMENT_SPLIT_BILL_RESTAURANT',
       'INVEST_IN_GOLD', 'JOINING_ANNIVERSARY', 'LOST_CARD_ORDER',
       'MADE_MONEY_REQUEST_NOT_SPLIT_BILL', 'METAL_RESERVE_PLAN',
       'NO_INITIAL_CARD_ORDER', 'NO_INITIAL_CARD_USE',
       'ONBOARDING_TIPS_ACTIVATED_USERS', 'PROMO', 'PROMO_CARD_ORDER',
       'REENGAGEMENT_ACTIVE_FUNDS', 'SILVER_ENGAGEMENT_FEES_SAVED',
       'SILVER_ENGAGEMENT_INACTIVE_CARD', 'WELCOME_BACK', 'EMAIL', 'PUSH',
       'SMS', 'FAILED', 'SENT']] = users_filtered[['count of notifications',
       'BLACK_FRIDAY', 'BLUE_TUESDAY', 'ENGAGEMENT_SPLIT_BILL_RESTAURANT',
       'INVEST_IN_GOLD', 'JOINING_ANNIVERSARY', 'LOST_CARD_ORDER',
       'MADE_MONEY_REQUEST_NOT_SPLIT_BILL', 'METAL_RESERVE_PLAN',
       'NO_INITIAL_CARD_ORDER', 'NO_INITIAL_CARD_USE',
       'ONBOARDING_TIPS_ACTIVATED_USERS', 'PROMO', 'PROMO_CARD_ORDER',
       'REENGAGEMENT_ACTIVE_FUNDS', 'SILVER_ENGAGEMENT_FEES_SAVED',
       'SILVER_ENGAGEMENT_INACTIVE_CARD', 'WELCOME_BACK', 'EMAIL', 'PUSH',
       'SMS', 'FAILED', 'SENT']].fillna(0) #fill these na values with 0

users_filtered.rename(
    columns={
        "BLACK_FRIDAY":"num_BLACK_FRIDAY_notification",
        "BLUE_TUESDAY":"num_BLUE_TUESDAY_notification",
        "ENGAGEMENT_SPLIT_BILL_RESTAURANT":"num_ENGAGEMENT_SPLIT_BILL_RESTAURANT_notification",
        "INVEST_IN_GOLD":"num_INVEST_IN_GOLD_notification",
        "JOINING_ANNIVERSARY":"num_JOINING_ANNIVERSARY_notification",
        "LOST_CARD_ORDER":"num_LOST_CARD_ORDER_notification",
        "MADE_MONEY_REQUEST_NOT_SPLIT_BILL":"num_MADE_MONEY_REQUEST_NOT_SPLIT_BILL_notification",
        "METAL_RESERVE_PLAN":"num_METAL_RESERVE_PLAN_notification",
        "NO_INITIAL_CARD_ORDER":"num_NO_INITIAL_CARD_ORDER_notification",
        "NO_INITIAL_CARD_USE":"num_NO_INITIAL_CARD_USE_notification",
        "ONBOARDING_TIPS_ACTIVATED_USERS":"num_ONBOARDING_TIPS_ACTIVATED_USERS_notification",
        "PROMO":"num_PROMO_notification",
        "PROMO_CARD_ORDER":"num_PROMO_CARD_ORDER_notification",
        "REENGAGEMENT_ACTIVE_FUNDS":"num_REENGAGEMENT_ACTIVE_FUNDS_notification",
        "SILVER_ENGAGEMENT_FEES_SAVED":"num_SILVER_ENGAGEMENT_FEES_SAVED_notification",
        "SILVER_ENGAGEMENT_INACTIVE_CARD":"num_SILVER_ENGAGEMENT_INACTIVE_CARD_notification",
        "WELCOME_BACK":"num_WELCOME_BACK_notification",
        "EMAIL":"num_EMAIL_notification",
        "PUSH":"num_PUSH_notification",
        "SMS":"num_SMS_notification",
        "FAILED":"num_FAILED_notification",
        "SENT":"num_SENT_notification",
        }
,inplace=True)

users_filtered.info()

users_filtered["latest_notification_date"]=pd.to_datetime(users_filtered["latest_notification_date"])

latest_max_notification_date=users_filtered["latest_notification_date"].max()

#based on the approach you want comment whichever of those two rows
#users_filtered["latest_notification_date"]=users_filtered["latest_notification_date"].fillna(latest_max_notification_date)
users_filtered["latest_notification_date"]=users_filtered["latest_notification_date"].fillna(users_filtered["created_date"])

users_filtered.head()

users_filtered.columns

users_filtered[['latest_notification_reason', 'latest_notification_channel', 'latest_notification_status']]=users_filtered[['latest_notification_reason', 'latest_notification_channel', 'latest_notification_status']].fillna("None")

users_filtered.info()

# create a new column to compare if the latest action was a notification received or a transaction by the user
users_filtered["Is_transactionlatest"]=users_filtered.apply(lambda x: 1 if x["latest_notification_date"] > x["last_transaction_date"] else 0,axis=1)

users_filtered.head()

users_filtered.info()

# create a new column with days between latest notification and transaction
users_filtered["days_bw_transaction_notif"]=abs((users_filtered["latest_notification_date"] - users_filtered["last_transaction_date"]).dt.days)

"""Notifications merge to users_filtered brand"""

devices_2 = devices.copy()

devices_2.head()

users_filtered=users_filtered.merge(devices_2,how="left",on="user_id")

users_filtered.info()

final_df = users_filtered.copy() # create a copy of the dataframe before modelling

final_df # we will examine columns to drop.
# anything with a date will be removed, user_id, as well as city as it has very many values inside which will make it difficult to enocde

final_df=final_df.drop(columns=["user_id","city","created_date","latest_notification_date","last_transaction_date"])

final_df.info()

"""Since the objective of the assignment is a churn prediction model, and we have decided to have 3 months without a transaction as the cuttoff point, we will first create our target column to label as "churned" vs "not-churned". We will then delete the days since last transaction column, as we want to be able to predict based on the remaining characteristics the churn risk."""

final_df["Is_Churned"]=final_df.apply(lambda x: 1 if x["days_since_last_transaction"] >=90 else 0,axis=1)

final_df=final_df.drop("days_since_last_transaction",axis=1)

pd.set_option('display.max_columns', 60)

final_df["country"].unique()

"""## 3. Encoding and Model Running"""

final_df_encoded=pd.get_dummies(final_df,drop_first=True) # one hot encoding categorical variables

X = final_df_encoded.drop("Is_Churned",axis=1)

y = final_df_encoded["Is_Churned"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y,shuffle=True)
# test size 25% of the sample, stratify = y because of the uneven labels of the y, and shuffle so that we mix the data

ss = StandardScaler() # importing a standard scaler to fit transform train and transform test

X_train = pd.DataFrame(ss.fit_transform(X_train),columns=X.columns)

X_test = pd.DataFrame(ss.transform(X_test),columns = X.columns)

classifiers = [
          LogisticRegression(class_weight="balanced",max_iter=10000),
          GaussianNB(),RandomForestClassifier(),KNeighborsClassifier()]
          # creating a list named classifiers that will contain 4 different classifiers to test performance.
# class weight balanced

cv=KFold(n_splits=5) # arbitrary choice of 5 splits for a more fair assesment and save it to the variable cv

for classifier in tqdm(classifiers):
    classifier.fit(X_train,y_train)
    print(classifier,"Training scores")
    recall_score = cross_val_score(classifier,X_train,y_train,cv=cv,scoring="recall")
    print(classifier, f"Recall score : {round(recall_score.mean(),2)}")
    f1_score = cross_val_score(classifier,X_train,y_train,cv=cv,scoring="f1")
    print(classifier, f"f1 score : {round(f1_score.mean(),2)}")
    baccuracy_score = cross_val_score(classifier,X_train,y_train,cv=cv,scoring="balanced_accuracy")
    print(classifier, f"Balanced accuracy score : {round(baccuracy_score.mean(),2)}")
    print()
    class_pred=classifier.predict(X_test)
    print("Test classification report:")
    print(classification_report(y_test,class_pred))
    print()
    print()

log_r = LogisticRegression(class_weight="balanced",max_iter=10000)

log_r.fit(X_train,y_train)

predictions_regular = log_r.predict(X_test)

print(classification_report(y_test,predictions_regular))

print(log_r.coef_)

importance = log_r.coef_[0]

logistic_feature_importance=pd.DataFrame({
    "Feature": X_train.columns,
    "Coefficient": importance
})

pd.set_option('display.max_rows', 50)

logistic_feature_importance["Absolute_val_coef"] = logistic_feature_importance["Coefficient"].abs()

logistic_feature_importance=logistic_feature_importance.sort_values(by="Absolute_val_coef",ascending=False)

logistic_feature_importance.head(20)

# higher age in service (days between created date and last transaction, stronger influence in reduction in churn)
# the same for the month of the year which is surprising i.e the higher the stronger the influence in churn reduction, could be higher LTV users recruited later on?
# the higher the time between transaction and notification the higher the influence in churn indicating the effect of reactivation via notifications



"""Two options available for next iteration ; can train only with top 10-15 columns and can train again using data available for 20 days within the service to prevent leakage for a stronger & more pratical churn prediction model"""